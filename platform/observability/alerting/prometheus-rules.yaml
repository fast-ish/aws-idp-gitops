apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: platform-alerts
  namespace: observability
  labels:
    app.kubernetes.io/name: platform-alerts
    app.kubernetes.io/part-of: observability
spec:
  groups:
    # Critical Infrastructure Alerts
    - name: infrastructure.critical
      interval: 30s
      rules:
        - alert: NodeNotReady
          expr: kube_node_status_condition{condition="Ready", status="true"} == 0
          for: 5m
          labels:
            severity: critical
            team: platform
          annotations:
            summary: "Node {{ $labels.node }} is not ready"
            description: "Node has been not ready for more than 5 minutes"
            runbook_url: "https://runbooks.internal/node-not-ready"

        - alert: NodeHighCPU
          expr: |
            100 - (avg by (node) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 90
          for: 10m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "Node {{ $labels.node }} has high CPU usage"
            description: "CPU usage is above 90% for more than 10 minutes"

        - alert: NodeHighMemory
          expr: |
            (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
          for: 10m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "Node {{ $labels.node }} has high memory usage"
            description: "Memory usage is above 90% for more than 10 minutes"

        - alert: PersistentVolumeFillingUp
          expr: |
            kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes < 0.1
          for: 5m
          labels:
            severity: critical
            team: platform
          annotations:
            summary: "PVC {{ $labels.persistentvolumeclaim }} is almost full"
            description: "Only {{ $value | humanizePercentage }} space remaining"

    # Pod Health Alerts
    - name: pods.health
      interval: 30s
      rules:
        - alert: PodCrashLooping
          expr: |
            rate(kube_pod_container_status_restarts_total[15m]) * 60 * 15 > 3
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
            description: "Pod has restarted {{ $value | printf \"%.0f\" }} times in 15 minutes"

        - alert: PodNotReady
          expr: |
            kube_pod_status_ready{condition="true"} == 0 and
            kube_pod_status_phase{phase="Running"} == 1
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is not ready"
            description: "Pod has been running but not ready for more than 10 minutes"

        - alert: ContainerOOMKilled
          expr: |
            kube_pod_container_status_last_terminated_reason{reason="OOMKilled"} == 1
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: "Container {{ $labels.container }} in {{ $labels.namespace }}/{{ $labels.pod }} was OOM killed"
            description: "Container ran out of memory and was terminated"

        - alert: HighContainerCPU
          expr: |
            sum(rate(container_cpu_usage_seconds_total{namespace=~"team-.*"}[5m])) by (namespace, pod, container)
            / sum(container_spec_cpu_quota{namespace=~"team-.*"} / container_spec_cpu_period) by (namespace, pod, container) > 0.9
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Container {{ $labels.container }} in {{ $labels.namespace }}/{{ $labels.pod }} has high CPU"
            description: "Container is using more than 90% of its CPU limit"

        - alert: HighContainerMemory
          expr: |
            container_memory_working_set_bytes{namespace=~"team-.*"}
            / container_spec_memory_limit_bytes{namespace=~"team-.*"} > 0.9
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Container {{ $labels.container }} in {{ $labels.namespace }}/{{ $labels.pod }} has high memory"
            description: "Container is using more than 90% of its memory limit"

    # Deployment Health Alerts
    - name: deployments.health
      interval: 30s
      rules:
        - alert: DeploymentReplicasMismatch
          expr: |
            kube_deployment_spec_replicas != kube_deployment_status_replicas_available
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has replica mismatch"
            description: "Deployment has not achieved desired replica count for 15 minutes"

        - alert: DeploymentGenerationMismatch
          expr: |
            kube_deployment_status_observed_generation != kube_deployment_metadata_generation
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has generation mismatch"
            description: "Deployment config change has not been applied"

        - alert: StatefulSetReplicasMismatch
          expr: |
            kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has replica mismatch"
            description: "StatefulSet has not achieved desired replica count for 15 minutes"

    # Platform Component Alerts
    - name: platform.components
      interval: 30s
      rules:
        - alert: BackstageDown
          expr: up{job="backstage"} == 0
          for: 5m
          labels:
            severity: critical
            team: platform
          annotations:
            summary: "Backstage is down"
            description: "Backstage developer portal is not responding"

        - alert: ArgoCDDown
          expr: up{job=~"argocd.*"} == 0
          for: 5m
          labels:
            severity: critical
            team: platform
          annotations:
            summary: "ArgoCD component is down"
            description: "ArgoCD {{ $labels.job }} is not responding"

        - alert: ArgoWorkflowsDown
          expr: up{job=~"argo-workflows.*"} == 0
          for: 5m
          labels:
            severity: critical
            team: platform
          annotations:
            summary: "Argo Workflows is down"
            description: "Argo Workflows {{ $labels.job }} is not responding"

        - alert: ArgoCDAppOutOfSync
          expr: |
            argocd_app_info{sync_status="OutOfSync"} == 1
          for: 30m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "ArgoCD app {{ $labels.name }} is out of sync"
            description: "Application has been out of sync for more than 30 minutes"

        - alert: ArgoCDAppHealthDegraded
          expr: |
            argocd_app_info{health_status=~"Degraded|Unknown"} == 1
          for: 10m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "ArgoCD app {{ $labels.name }} health is {{ $labels.health_status }}"
            description: "Application health has been degraded for more than 10 minutes"

    # Security Alerts
    - name: security.alerts
      interval: 30s
      rules:
        - alert: FalcoCriticalAlert
          expr: |
            sum(increase(falco_events_total{priority="Critical"}[5m])) > 0
          for: 0m
          labels:
            severity: critical
            team: security
          annotations:
            summary: "Falco detected critical security event"
            description: "Critical security event detected in the cluster"

        - alert: CriticalVulnerabilityDetected
          expr: |
            sum(trivy_image_vulnerabilities{severity="Critical"}) by (image_repository, image_tag) > 0
          for: 10m
          labels:
            severity: warning
            team: security
          annotations:
            summary: "Critical vulnerability in {{ $labels.image_repository }}:{{ $labels.image_tag }}"
            description: "Image has {{ $value }} critical vulnerabilities"

        - alert: KyvernoPolicyViolation
          expr: |
            sum(increase(kyverno_policy_results_total{rule_result="fail"}[1h])) by (policy_name) > 10
          for: 5m
          labels:
            severity: warning
            team: security
          annotations:
            summary: "High number of Kyverno policy violations"
            description: "Policy {{ $labels.policy_name }} has had {{ $value }} violations in the last hour"

    # SLO Alerts
    - name: slo.alerts
      interval: 60s
      rules:
        - alert: HighErrorRate
          expr: |
            sum(rate(http_requests_total{status=~"5.."}[5m]))
            / sum(rate(http_requests_total[5m])) > 0.05
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "High error rate detected"
            description: "Error rate is {{ $value | humanizePercentage }} (>5%)"

        - alert: HighLatencyP99
          expr: |
            histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) > 1
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "High P99 latency detected"
            description: "P99 latency is {{ $value | humanizeDuration }} (>1s)"

    # Certificate Alerts
    - name: certificates.alerts
      interval: 60s
      rules:
        - alert: CertificateExpiringSoon
          expr: |
            certmanager_certificate_expiration_timestamp_seconds - time() < 86400 * 14
          for: 1h
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "Certificate {{ $labels.name }} expires in less than 14 days"
            description: "Certificate will expire in {{ $value | humanizeDuration }}"

        - alert: CertificateExpiryCritical
          expr: |
            certmanager_certificate_expiration_timestamp_seconds - time() < 86400 * 3
          for: 10m
          labels:
            severity: critical
            team: platform
          annotations:
            summary: "Certificate {{ $labels.name }} expires in less than 3 days"
            description: "Certificate will expire in {{ $value | humanizeDuration }}"
